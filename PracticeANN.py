# -*- coding: utf-8 -*-
"""In Class Assignment - ANN digits dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmvocpBHGfzYj8ZP4kXZ6pCdHuGx3pjC
"""

import seaborn
print(seaborn.__version__) # make sure you're on 13

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, Input
# these three lines are essential for any NN
# tensor is anything that is scalable like a vector or matrix. tensor flow means that data type is flowing. tensor is a vector.

# Commented out IPython magic to ensure Python compatibility.
from tensorflow import keras
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

len(X_train)

X_train.shape

X_train[3].shape

X_train[3]

plt.matshow(X_train[0])

X_train_flattened = X_train.reshape(len(X_train), 28*28)
X_test_flattened = X_test.reshape(len(X_test), 28*28)

# model = keras.Sequential([
    #keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
#]
#)

model = keras.Sequential()
model.add(Input(shape=(X_train_flattened.shape[1],)))
model.add(layers.Dense(10, activation='sigmoid'))

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs=5)
# y is alr vector so we dont need to flatten it
# gradient descent is backward propogation, epoch is forward

model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='tanh')
]
)

model.compile(
    optimizer = 'adam',
    loss = 'SparseCategoricalCrossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs=5)

model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='relu')
]
)

model.compile(
    optimizer = 'adam',
    loss = 'SparseCategoricalCrossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs=5)

X_train_flattened = X_train_flattened/255
X_test_flattened = X_test_flattened/255

X_train_flattened[3]

model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='relu')
]
)

model.compile(
    optimizer = 'adam',
    loss = 'SparseCategoricalCrossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs=5)

model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
]
)

model.compile(
    optimizer = 'adam',
    loss = 'SparseCategoricalCrossentropy',
    metrics = ['accuracy']
)

model.fit(X_train_flattened, y_train, epochs=5)

model.evaluate(X_test_flattened, y_test)

plt.matshow(X_test[15])

y_predicted = model.predict(X_test_flattened)

y_predicted[15]

np.argmax(y_predicted[15])

y_predicted_labels = [np.argmax(i) for i in y_predicted]
cm = tf.math.confusion_matrix(labels=y_test, predictions = y_predicted_labels)
cm

import seaborn as sns
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

import seaborn
print(seaborn.__version__) # make sure you're on 13

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models, Input

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

model = keras.Sequential()
model.add(Input(shape=(X_train_flattened.shape[1],)))
model.add(layers.Dense(10, activation='sigmoid'))